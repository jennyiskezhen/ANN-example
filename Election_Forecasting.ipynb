{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# basic solution\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# creative solution\n",
    "from numpy import vstack\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_accuracy(pred, true):\n",
    "    assert(len(pred) == len(true))\n",
    "    num_labels = len(true)\n",
    "    num_pos = sum(true)\n",
    "    num_neg = num_labels - num_pos\n",
    "    frac_pos = num_pos/num_labels\n",
    "    weight_pos = 1/frac_pos\n",
    "    weight_neg = 1/(1-frac_pos)\n",
    "    num_pos_correct = 0\n",
    "    num_neg_correct = 0\n",
    "    for pred_i, true_i in zip(pred, true):\n",
    "        num_pos_correct += (pred_i == true_i and true_i == 1)\n",
    "        num_neg_correct += (pred_i == true_i and true_i == 0)\n",
    "    weighted_accuracy = ((weight_pos * num_pos_correct) \n",
    "                         + (weight_neg * num_neg_correct))/((weight_pos * num_pos) + (weight_neg * num_neg))\n",
    "    return weighted_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Feature Extraction:</h3><p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the csv files\n",
    "df_train = pd.read_csv(\"train_2016.csv\", sep=',', encoding='unicode_escape', thousands=',')\n",
    "df_test = pd.read_csv(\"test_2016_no_label.csv\", sep=',', encoding='unicode_escape', thousands=',')\n",
    "del df_train['County']\n",
    "del df_test['County']\n",
    "\n",
    "# get true labels\n",
    "n,d = df_train.shape\n",
    "y = np.ones(n)\n",
    "y[df_train.DEM < df_train.GOP] = 0\n",
    "\n",
    "# delete unneeded columns\n",
    "del df_train['DEM']\n",
    "del df_train['GOP']\n",
    "\n",
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_train.values)\n",
    "X_training = scaler.transform(df_train.values)\n",
    "X_testing = scaler.transform(df_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of X_training is: (1555, 7)\n",
      "The size of X_testing is: (1555, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of X_training is:\", X_training.shape)\n",
    "print(\"The size of X_testing is:\", X_testing.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Use Three Training Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"knn_3uni\": KNeighborsClassifier(n_neighbors=3, weights='uniform'),\n",
    "    \"knn_3dis\": KNeighborsClassifier(n_neighbors=3, weights='distance'),\n",
    "    \"knn_5uni\": KNeighborsClassifier(n_neighbors=5, weights='uniform'),\n",
    "    \"knn_5dis\": KNeighborsClassifier(n_neighbors=5, weights='distance'),\n",
    "    \"knn_9uni\": KNeighborsClassifier(n_neighbors=9, weights='uniform'),\n",
    "    \"knn_9dis\": KNeighborsClassifier(n_neighbors=9, weights='distance'),\n",
    "    \"knn_13uni\": KNeighborsClassifier(n_neighbors=13, weights='uniform'),\n",
    "    \"knn_13dis\": KNeighborsClassifier(n_neighbors=13, weights='distance'),\n",
    "    \n",
    "    \"lda_svd\": LinearDiscriminantAnalysis(solver='svd'),\n",
    "    \"lda_lsqr\": LinearDiscriminantAnalysis(solver='lsqr'),\n",
    "    \"lda_eigen\": LinearDiscriminantAnalysis(solver='eigen'),\n",
    "\n",
    "    \"svc_1lin\": SVC(kernel='linear', C=1),\n",
    "    \"svc_10lin\": SVC(kernel='linear', C=10),\n",
    "    \"svc_100lin\": SVC(kernel='linear', C=100),\n",
    "    \"svc_1rbf\": SVC(kernel='rbf', C=1),\n",
    "    \"svc_10rbf\": SVC(kernel='rbf', C=10),\n",
    "    \"svc_100rbf\": SVC(kernel='rbf', C=100),\n",
    "    \"svc_1poly\": SVC(kernel='poly', C=1),\n",
    "    \"svc_10poly\": SVC(kernel='poly', C=10),\n",
    "    \"svc_100poly\": SVC(kernel='poly', C=100)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Training and Validation\n",
    "Split the data to a training set and validation set or performing a cross-validation for model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn_3uni : 0.692\n",
      "knn_3dis : 0.694\n",
      "knn_5uni : 0.684\n",
      "knn_5dis : 0.691\n",
      "knn_9uni : 0.669\n",
      "knn_9dis : 0.674\n",
      "knn_13uni : 0.661\n",
      "knn_13dis : 0.674\n",
      "lda_svd : 0.69\n",
      "lda_lsqr : 0.69\n",
      "lda_eigen : 0.69\n",
      "svc_1lin : 0.642\n",
      "svc_10lin : 0.645\n",
      "svc_100lin : 0.645\n",
      "svc_1rbf : 0.681\n",
      "svc_10rbf : 0.718\n",
      "svc_100rbf : 0.715\n",
      "svc_1poly : 0.659\n",
      "svc_10poly : 0.699\n",
      "svc_100poly : 0.707\n"
     ]
    }
   ],
   "source": [
    "# set up dictionary to hold the cross-validation scores for each model\n",
    "scores = {}\n",
    "for m in models.keys(): \n",
    "    scores[m] = []\n",
    "\n",
    "# cross-validation on 10 folds\n",
    "kf10 = KFold(n_splits=10,shuffle=True)\n",
    "for train_index, test_index in kf10.split(X_training):\n",
    "    X_train, X_test = X_training[train_index], X_training[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        scores[name].append(weighted_accuracy(predictions, y_test))\n",
    "\n",
    "# print the mean score for each model\n",
    "for n, s in scores.items(): \n",
    "    print(n,':',np.round(np.mean(np.array(s)),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-nearest neighbors (k-NN), linear discriminant analysis (LDA), and support vector machine (SVM) learning methods were chosen because they seemed suitable for a binary classification where the features aren't discrete. Each learning method has potentioal strengths and weaknesses. k-NN doesn't depend on the data being separable like the other two but it can be sensitive to outliers. LDA assumes that the data forms a spherical normal distribution but it takes the entire feature matrix into account so it might be better than the SVM in that aspect. Meanwhile the SVM might be better than the LDA in that it only depends on the support vectors and the use of kernels means the separation of different classes doesn't have to be linear unlike for LDA.\n",
    "\n",
    "For the k-NN classifier, a range of neighbors from 3 to 13 were selected, and the neighbors were weighted uniformly or by distance. For LDA, different solvers were tested. For the SVM, the linear, rbf, and polynomial kernels were tested, as well as a range of 1 to 100 for C value. Models were compared using 10-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Artifical Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, csv_file, mode = 'train'):\n",
    "        self.mode = mode\n",
    "        df = pd.read_csv(csv_file, sep=',',encoding='unicode_escape',thousands=',')\n",
    "        del df['County']\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            # labels        \n",
    "            n,d = df.shape\n",
    "            y = np.ones(n)\n",
    "            y[df.DEM < df.GOP] = 0\n",
    "            del df['DEM']\n",
    "            del df['GOP']\n",
    "            del df['FIPS']\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(df.values)\n",
    "\n",
    "            self.X = scaler.transform(df.values)\n",
    "            self.y = y.reshape(n,1)\n",
    "        else:\n",
    "            del df['FIPS']\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(df.values)\n",
    "            \n",
    "            self.X = scaler.transform(df.values)\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        # Returns count of samples (an integer) \n",
    "#         raise NotImplementedError\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Given an index, returns the correponding datapoint. \n",
    "        # This function is called from dataloader like this:\n",
    "        # img, label = CSVDataset.__getitem__(99)  # For 99th item\n",
    "#         raise NotImplementedError\n",
    "        # labels\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            X = torch.Tensor(self.X[idx]).float()\n",
    "            y = torch.Tensor(self.y[idx]).float()\n",
    "            return X,y\n",
    "        else:\n",
    "            X = torch.Tensor(self.X[idx]).float()\n",
    "            return X\n",
    "    \n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,dim):\n",
    "        \"\"\"\n",
    "        initialize most NN layers here.\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "\n",
    "        self.fc1 = nn.Linear(dim, 160)\n",
    "        self.b1 = nn.BatchNorm1d(160)\n",
    "        self.fc2 = nn.Linear(160, 64)\n",
    "        self.b2 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 8)\n",
    "        self.b3 = nn.BatchNorm1d(8)\n",
    "        self.fc4 = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define in what order the input x is forwarded through all the NN layers to become a final output. \n",
    "        \"\"\"        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.b1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.b2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.b3(x)\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def train(csv_file,dim):\n",
    "    # Initialize an object of the model class\n",
    "    net = Net(dim).to(device)\n",
    "    # Define loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    # Create optimizer\n",
    "    # optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "    # Initialize an object of the dataset class\n",
    "    dataset = CSVDataset(csv_file)\n",
    "    train, test = dataset.get_splits()\n",
    "    # Wrap a dataloader around the dataset object.\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=311, shuffle=True)\n",
    "    # Beging training!\n",
    "    for batch_idx, (input, target) in enumerate(train_dl):\n",
    "        # Always want to use zero_grad(), backward(), and step() in the following order.\n",
    "        # zero_grad clears old gradients from the last step (otherwise you’d just accumulate the gradients from all loss.backward() calls).\n",
    "        optimizer.zero_grad()\n",
    "        # As said before, only code as below if the network belongs to the nn.Module class.\n",
    "        input = input.view(-1,dim)\n",
    "        output = net(input)        \n",
    "        loss = criterion(output, target)        \n",
    "        # loss.backward() computes the derivative of the loss w.r.t. the parameters (or anything requiring gradients) using backpropagation.\n",
    "        loss.backward()\n",
    "        # optimizer.step() causes the optimizer to take a step based on the gradients of the parameters.\n",
    "        optimizer.step()\n",
    "    \n",
    "    #validate \n",
    "    predictions, actuals = list(), list()\n",
    "    for batch_idx, (input, target) in enumerate(test_dl):\n",
    "        input = input.view(-1,dim)\n",
    "        output = net(input)\n",
    "        \n",
    "        yhat = torch.round(output)\n",
    "#         yhat = output\n",
    "    \n",
    "        yhat = yhat.detach().numpy()\n",
    "        \n",
    "        actual = target.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        \n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "        \n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    \n",
    "#     # accuracy\n",
    "    acc = weighted_accuracy(predictions, actuals)    \n",
    "        \n",
    "    return loss, predictions, acc, net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 New features using train_2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016 = pd.read_csv(\"train_2016.csv\", sep=',',encoding='unicode_escape',thousands=',')\n",
    "df_2012 = pd.read_csv(\"train_2012.csv\", sep=',',encoding='unicode_escape',thousands=',')\n",
    "\n",
    "df_mean0 = (df_2016.iloc[:,2:]+df_2012.iloc[:,2:])/2\n",
    "df_mean = df_2016.iloc[:,:2].join(df_mean0)\n",
    "df_mean.to_csv(\"train_mean.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 New features using graph.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph = pd.read_csv(\"graph.csv\", sep=',',encoding='unicode_escape',thousands=',')\n",
    "df_train2016 = pd.read_csv(\"train_2016.csv\", sep=',',encoding='unicode_escape',thousands=',')\n",
    "n,d = df_graph.shape\n",
    "for i in range(n):\n",
    "    index_2 = df_train2016.FIPS.values == df_graph.DST.iloc[i]        \n",
    "    if any(index_2): \n",
    "        df_graph.loc[i,'DST_C'] = df_train2016.DEM.values[index_2]\n",
    "    else:\n",
    "        df_graph.loc[i,'DST_C'] = np.nan\n",
    "    \n",
    "df_graph.dropna(axis = 0, how = 'any', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6176, 11)\n"
     ]
    }
   ],
   "source": [
    "# new df_graph\n",
    "n,d = df_graph.shape\n",
    "\n",
    "df1 = pd.DataFrame([])\n",
    "for i in range(n):\n",
    "    index1 = df_train2016.FIPS.values == df_graph.DST.iloc[i]\n",
    "    A = df_train2016.loc[index1,:].values[0]\n",
    "    df1[i] = np.concatenate([df_graph.iloc[i,:], A])\n",
    "#     df_graph.iloc[i].merge(A,left_index=True, right_index=True)\n",
    "\n",
    "df_new = df1.T\n",
    "df_new.columns = ['SRC','a','b','DST','County','DEM','GOP','MedianIncome','MigraRate','BirthRate','DeathRate','BachelorRate','UnemploymentRate']\n",
    "del df_new['a']\n",
    "del df_new['b']\n",
    "\n",
    "# df_new['SRC'].astype(int)\n",
    "df_new.to_csv('train2016_new.csv',index=False)\n",
    "print(df_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2686, 10)\n"
     ]
    }
   ],
   "source": [
    "df_new = pd.read_csv(\"train2016_new.csv\", sep=',',encoding='unicode_escape',thousands=',')\n",
    "df1 = df_new.groupby('SRC')[['DST','DEM','GOP']].sum().reset_index();\n",
    "df2 = df_new.groupby('SRC')[['MedianIncome','MigraRate','BirthRate','DeathRate','BachelorRate','UnemploymentRate']].mean().reset_index();\n",
    "\n",
    "del df2['SRC']\n",
    "df = df1.join(df2)\n",
    "df=df.rename(columns = {'SRC':'FIPS','DST':'County'})\n",
    "df.to_csv('train2016_new.csv',index=False,float_format='%.3f')\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation: [0.84398341]\n"
     ]
    }
   ],
   "source": [
    "dim_input = 6\n",
    "EPOCHS = 200\n",
    "save_file = \"best_model.pt\"\n",
    "\n",
    "best_val_acc = float(\"-inf\")\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    loss, y_predict, val_acc, net = train(\"train2016_new.csv\",dim_input)    \n",
    "    if val_acc > best_val_acc:        \n",
    "        best_val_acc = val_acc\n",
    "        torch.save(net.state_dict(), save_file)\n",
    "print(\"best validation:\",best_val_acc)    \n",
    "   \n",
    "model = Net(dim_input);\n",
    "best_val_acc_model_weights = torch.load(save_file);\n",
    "model.load_state_dict(best_val_acc_model_weights);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 test 2016 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"test_2016_no_label.csv\"\n",
    "dataset = CSVDataset(csv_file, mode = 'test')\n",
    "test_X = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "predictions = list()\n",
    "for batch_idx, input in enumerate(test_X):\n",
    "    input = input.view(-1,dim_input)        \n",
    "    output = model(input)\n",
    "    yhat = torch.round(output)\n",
    "#     yhat = output\n",
    "    yhat = yhat.detach().numpy()\n",
    "        \n",
    "    predictions.append(yhat)\n",
    "        \n",
    "predictions = vstack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sampleSubmission.csv', sep=',',encoding='unicode_escape',thousands=',')\n",
    "\n",
    "df['Result'] = predictions.astype(int)\n",
    "# df['Result'] = predictions\n",
    "df.to_csv('output.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graph = pd.read_csv(\"graph.csv\", sep=',',encoding='unicode_escape',thousands=',')\n",
    "df_test = pd.read_csv(\"test_2016_no_label.csv\", sep=',',encoding='unicode_escape',thousands=',')\n",
    "\n",
    "n,d = df_graph.shape\n",
    "for i in range(n):\n",
    "    index_2 = df_test.FIPS.values == df_graph.DST.iloc[i]\n",
    "        \n",
    "    if any(index_2): \n",
    "        df_graph.loc[i,'DST_C'] = predictions[index_2][0]\n",
    "    else:\n",
    "        df_graph.loc[i,'DST_C'] = np.nan\n",
    "\n",
    "df_graph.dropna(axis = 0, how = 'any', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_major(grp):\n",
    "#     grp['Class_major'] = grp['DST_C'].agg(lambda x: pd.Series.mode(x)[0])\n",
    "#     return grp\n",
    "# class_major = df_graph.groupby('SRC').apply(add_major);\n",
    "\n",
    "def add_major(grp):    \n",
    "    if grp['SRC'].iloc[0] == grp['DST'].iloc[0]:\n",
    "        a = grp['DST_C'].iloc[0]\n",
    "        b = grp['DST_C'].iloc[1:].mean()\n",
    "    else:\n",
    "        a = grp['DST_C'].values.mean()\n",
    "        b = a\n",
    "        \n",
    "    if (len(grp['DST_C'].values)>1):\n",
    "        grp['Class_major'] = (a+b)/2\n",
    "    else:\n",
    "        grp['Class_major'] = a\n",
    "#     grp['Class_major'] = grp['DST_C'].mean()\n",
    "    return grp\n",
    "\n",
    "class_major = df_graph.groupby('SRC').apply(add_major);\n",
    "class_major['class'] = np.round(class_major.Class_major.values).astype(int)\n",
    "class_major.to_csv('dis_output.csv',index=False,float_format='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = class_major.groupby('DST')['Class_major'].mean().reset_index();\n",
    "df_final = pd.read_csv(\"output.csv\", sep=',',encoding='unicode_escape',thousands=',')\n",
    "n, d = df_final.shape\n",
    "\n",
    "for j in range(n):\n",
    "    index1 = df.DST.values == df_final.FIPS.iloc[j]    \n",
    "    df_final.loc[j,'Result'] = df.Class_major.iloc[index1].values\n",
    "\n",
    "df_final.Result = np.round(df_final.Result.values).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Test the model using full train_2016 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final R2:  [0.805]\n"
     ]
    }
   ],
   "source": [
    "csv_file = \"train_2016.csv\"\n",
    "dataset = CSVDataset(csv_file)\n",
    "test_X = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "predictions, actuals = list(),list()\n",
    "for batch_idx, (input,target) in enumerate(test_X):\n",
    "    input = input.view(-1,dim_input)        \n",
    "    # validate the model\n",
    "    output = model(input)\n",
    "    yhat = torch.round(output)\n",
    "    yhat = yhat.detach().numpy()\n",
    "    \n",
    "    actual = target.numpy()\n",
    "    actual = actual.reshape((len(actual), 1))\n",
    "        \n",
    "    predictions.append(yhat)\n",
    "    actuals.append(actual)\n",
    "        \n",
    "predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "acc = weighted_accuracy(predictions, actuals)\n",
    "print(\"Final R2: \", np.round(acc,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R2 values is around 0.8 for the model performance, which is well predicted using an ANN model built from PyTorch package. The Neural Network has 5 hidden layers, the number of nodes in the first hidden layer are increased to 160 and then are gradually decreased to 1, which makes the model more expressive. The operation from layer to layer is a linear model and the activation functions for the 3 hidden layers are ReLu while at the output layer the activation function is sigmoid, so that to obtain outputs between 0 and 1. The final output will be the rounded values of 0 or 1. The ANN model was trained using 2/3 of the training data and validated using the rest of the 1/3 data. Adam optimizer was used to update the parameters. \n",
    "\n",
    "When training the model, a batch size of 32 was used to make the model more dynamics. When validating the model,a batch size of 311 was used to make it smoother. 200 epoches were used and in each epoch the accuracy of the validation was calculated using the weighted_accuracy function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
